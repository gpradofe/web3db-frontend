---
slug: /main-db-components/db-engine
sidebar_position: 4
---

import Admonition from "@theme/Admonition";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import { Mermaid } from "mdx-mermaid/Mermaid";

# Database Engine

## Database Engine: Bridging IPFS and Hive

The database engine is a critical component of the QueryAPIKernel solution, serving as a bridge between the InterPlanetary File System (IPFS) and Apache Hive. It provides an API for executing SQL queries, managing database state, and interacting with IPFS for data persistence.

<Admonition type="info">
  The database engine is implemented as a Flask application, leveraging the
  Flask-RestX framework for building a RESTful API.
</Admonition>

## Architecture Overview

The database engine consists of the following key components:

- **Flask Application**: The main entry point of the database engine, handling HTTP requests and routing them to the appropriate endpoints.
- **IPFS Manager**: Responsible for uploading and fetching data from IPFS, ensuring data persistence and retrieval.
- **Query Engine**: Executes SQL queries using Apache Spark and Hive, providing a powerful and scalable query execution environment.

Here's a high-level architecture diagram illustrating the interaction between the components:

<Mermaid
  chart={`graph TD;
    A[Client] --> B[Flask Application]
    B --> C[IPFS Manager]
    B --> D[Query Engine]
    C --> E[IPFS]
    D --> F[Apache Spark]
    D --> G[Apache Hive]
`}
/>

## API Endpoints

The database engine exposes a RESTful API for executing queries and managing the database state. The main endpoint is:

- **POST /query**: Executes a SQL query and returns the results. It expects a JSON payload containing the query and an optional IPFS hash representing the existing database state.

<Tabs>
 <TabItem value="request" label="Request">
   ```json
      {
     "query": "SELECT * FROM users",
     "hash": "QmAbCdEfGhIjKlMnOpQrStUvWxYz"
   }
   ```

 </TabItem>
 <TabItem value="response" label="Response">
   ```json
   {
     "message": "Query executed successfully, SELECT operation",
     "data": [
       {
         "id": 1,
         "name": "John Doe",
         "email": "john@example.com"
       },
       {
         "id": 2,
         "name": "Jane Smith",
         "email": "jane@example.com"
       }
     ],
     "hash": "QmAbCdEfGhIjKlMnOpQrStUvWxYz"
   }
   ```
 </TabItem>
</Tabs>

## Query Execution Flow

When a query is received by the database engine, it follows the following flow:

1. The Flask application receives the query and the optional IPFS hash through the `/query` endpoint.
2. If an IPFS hash is provided (excluding the dummy hash), the IPFS Manager fetches the existing database state from IPFS.
3. The Query Engine initializes a SparkSession and establishes a connection with Apache Hive.
4. If a database state is fetched from IPFS, the Query Engine executes the SQL statements from the fetched dump to recreate the database state.
5. The Query Engine executes the received query using Spark SQL.
6. If the query is a non-SELECT statement (e.g., INSERT, UPDATE, DELETE), the modified database state is uploaded to IPFS using the IPFS Manager, and a new IPFS hash is generated.
7. The query results, along with the IPFS hash (if applicable), are returned as a JSON response.

<Admonition type="info">
  The database engine uses a temporary Hive warehouse directory to store the
  intermediate data during query execution. The warehouse directory is cleared
  before each query to ensure a clean state.
</Admonition>

## IPFS Integration

The database engine integrates with IPFS to store and retrieve database state snapshots. When a non-SELECT query modifies the database state, the updated state is serialized into a SQL dump and uploaded to IPFS. The generated IPFS hash serves as a reference to the database state at that point in time.

When executing a query, if an IPFS hash is provided, the database engine fetches the corresponding SQL dump from IPFS and executes the statements to recreate the database state before executing the actual query.

```python
def upload_to_ipfs(data):
   files = {'file': ('dump.sql', data)}
   response = requests.post(f"{IPFS_API_URL}/api/v0/add", files=files)
   if response.status_code == 200:
       ipfs_hash = response.json().get("Hash")
       return ipfs_hash
   else:
       raise ConnectionError(f"Failed to upload data to IPFS. Status code: {response.status_code}")

def fetch_sql_dump_from_ipfs(data_hash):
   response = requests.post(f"{IPFS_API_URL}/api/v0/cat?arg={data_hash}")
   if response.status_code == 200:
       return response.text
   else:
       raise ConnectionError(f"Failed to fetch data from IPFS. Status code: {response.status_code}")

```

## Examples

Here are a few examples demonstrating the usage of the database engine API:

<Tabs>
 <TabItem value="create-table" label="Create Table">
   ```bash
      curl -X POST -H "Content-Type: application/json" -d '{
     "query": "CREATE TABLE users (id INT, name STRING, email STRING)",
     "hash": "dummy_ipfs_hash"
   }' http://localhost:5000/query
   ```
   Response:
   ```json
   {
     "message": "Query executed successfully, non-SELECT operation",
     "data": [],
     "hash": "QmAbCdEfGhIjKlMnOpQrStUvWxYz"
   }
   ```
 </TabItem>
 <TabItem value="insert-data" label="Insert Data">
   ```bash
   curl -X POST -H "Content-Type: application/json" -d '{
     "query": "INSERT INTO users VALUES (1, \"John Doe\", \"john@example.com\"), (2, \"Jane Smith\", \"jane@example.com\")",
     "hash": "QmAbCdEfGhIjKlMnOpQrStUvWxYz"
   }' http://localhost:5000/query
   ```
   Response:
```json
   {
     "message": "Query executed successfully, non-SELECT operation",
     "data": [],
     "hash": "QmXyZaBcDeFgHiJkLmNoPqRsTuVwXyZ"
   }
   ```
 </TabItem>
 <TabItem value="select-data" label="Select Data">
```bash
   curl -X POST -H "Content-Type: application/json" -d '{
     "query": "SELECT * FROM users",
     "hash": "QmXyZaBcDeFgHiJkLmNoPqRsTuVwXyZ"
   }' http://localhost:5000/query
```
   Response:
```json
   {
     "message": "Query executed successfully, SELECT operation",
     "data": [
       {
         "id": 1,
         "name": "John Doe",
         "email": "john@example.com"
       },
       {
         "id": 2,
         "name": "Jane Smith",
         "email": "jane@example.com"
       }
     ],
     "hash": "QmXyZaBcDeFgHiJkLmNoPqRsTuVwXyZ"
   }
```
</TabItem>
</Tabs>

## Error Handling

The database engine handles errors gracefully and returns appropriate error responses. If an error occurs during query execution or IPFS interaction, an error response is returned with a corresponding status code and error message.

```python
except Exception as e:
   error_response = {
       "message": "An error occurred",
       "error": str(e),
       "data": None,
       "hash": None
   }
   return make_response(error_response, 413)
```

## Conclusion

The database engine is a crucial component that enables seamless integration between IPFS and Apache Hive, providing a powerful and flexible query execution environment. By leveraging IPFS for data persistence and Spark SQL for query processing, the database engine offers a scalable and efficient solution for managing and querying data in a decentralized manner.

With its RESTful API and support for SQL queries, the database engine simplifies the interaction with the underlying data storage and processing components, making it easier to build applications on top of the QueryAPIKernel solution.
