---
slug: /main-db-components/docker-compose
sidebar_position: 3
---

import CodeBlock from "@theme/CodeBlock";
import Admonition from "@theme/Admonition";

# Docker Compose

## Understanding the Docker Compose Configuration

The `docker-compose.yml` file is used to define and configure the services that make up the QueryAPIKernel solution. It specifies the images, containers, networks, and volumes required to run the system.

Let's break down the main components and their configurations:

## IPFS

The IPFS service is based on the `ipfs/go-ipfs:latest` image. It is configured with the following properties:

- Environment variables:
  - `IPFS_PROFILE=server`: Sets the IPFS profile to "server" mode.
  - `IPFS_PATH=/ipfsdata`: Specifies the path where IPFS data will be stored.
- Volumes:
  - `./data/ipfs:/ipfsdata`: Mounts the local `./data/ipfs` directory to the `/ipfsdata` directory in the container.
- Ports:
  - `4001:4001`: Exposes the IPFS swarm port.
  - `127.0.0.1:8080:8080`: Exposes the IPFS gateway port.
  - `127.0.0.1:8083:8081`: Exposes the IPFS API port.
  - `127.0.0.1:5003:5001`: Exposes the IPFS API port.

## Hadoop

The Hadoop ecosystem consists of several services, including HDFS (NameNode and DataNode) and YARN (ResourceManager, NodeManager, and HistoryServer).

### NameNode

The NameNode service is based on the `bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8` image. It is configured with the following properties:

- Container name: `namenode`
- Restart policy: `always`
- Ports:
  - `9870:9870`: Exposes the NameNode web UI port.
  - `9001:9000`: Exposes the NameNode IPC port.
- Volumes:
  - `hadoop_namenode:/hadoop/dfs/name`: Mounts the `hadoop_namenode` volume to store NameNode data.
- Environment variables:
  - `CLUSTER_NAME=test`: Sets the Hadoop cluster name.
  - `CORE_CONF_fs_defaultFS=hdfs://namenode:9000`: Configures the default HDFS file system URI.
- Environment file:
  - `./hadoop.env`: Loads environment variables from the `hadoop.env` file.

### DataNode

The DataNode service is based on the `bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8` image. It is configured with the following properties:

- Container name: `datanode`
- Restart policy: `always`
- Volumes:
  - `hadoop_datanode:/hadoop/dfs/data`: Mounts the `hadoop_datanode` volume to store DataNode data.
- Environment variables:
  - `SERVICE_PRECONDITION="namenode:9870"`: Specifies the NameNode service as a precondition.
  - `CORE_CONF_fs_defaultFS=hdfs://namenode:9000`: Configures the default HDFS file system URI.
- Ports:
  - `9864:9864`: Exposes the DataNode web UI port.
- Environment file:
  - `./hadoop.env`: Loads environment variables from the `hadoop.env` file.

<Admonition type="info">
  The ResourceManager, NodeManager, and HistoryServer services are configured
  similarly to the NameNode and DataNode services. They are based on the
  respective Hadoop images and have specific environment variables, volumes, and
  ports configured.
</Admonition>

## Spark

The Spark ecosystem consists of the Spark Master and Spark Worker services.

### Spark Master

The Spark Master service is based on the `bde2020/spark-master:3.0.0-hadoop3.2` image. It is configured with the following properties:

- Container name: `spark-master`
- Depends on:
  - `namenode`
  - `datanode`
- Ports:
  - `8082:8080`: Exposes the Spark Master web UI port.
  - `7077:7077`: Exposes the Spark Master port for communication with workers.
- Environment variables:
  - `INIT_DAEMON_STEP=setup_spark`: Specifies the initialization step for Spark.
  - `CORE_CONF_fs_defaultFS=hdfs://namenode:9000`: Configures the default HDFS file system URI.

### Spark Worker

The Spark Worker service is based on the `bde2020/spark-worker:3.0.0-hadoop3.2` image. It is configured with the following properties:

- Container name: `spark-worker-1`
- Depends on:
  - `spark-master`
- Ports:
  - `8081:8081`: Exposes the Spark Worker web UI port.
- Environment variables:
  - `SPARK_MASTER=spark://spark-master:7077`: Specifies the Spark Master URL.
  - `CORE_CONF_fs_defaultFS=hdfs://namenode:9000`: Configures the default HDFS file system URI.

## Hive

The Hive ecosystem consists of the Hive Server, Hive Metastore, and Hive Metastore PostgreSQL services.

### Hive Server

The Hive Server service is based on the `bde2020/hive:2.3.2-postgresql-metastore` image. It is configured with the following properties:

- Container name: `hive-server`
- Depends on:
  - `namenode`
  - `datanode`
- Environment file:
  - `./hadoop-hive.env`: Loads environment variables from the `hadoop-hive.env` file.
- Environment variables:
  - `HIVE_CORE_CONF_javax_jdo_option_ConnectionURL="jdbc:postgresql://hive-metastore/metastore"`: Configures the Hive metastore connection URL.
  - `SERVICE_PRECONDITION="hive-metastore:9083"`: Specifies the Hive Metastore service as a precondition.
- Ports:
  - `10000:10000`: Exposes the Hive Server port.

### Hive Metastore

The Hive Metastore service is based on the `bde2020/hive:2.3.2-postgresql-metastore` image. It is configured with the following properties:

- Container name: `hive-metastore`
- Environment file:
  - `./hadoop-hive.env`: Loads environment variables from the `hadoop-hive.env` file.
- Command: `/opt/hive/bin/hive --service metastore`
- Environment variables:
  - `SERVICE_PRECONDITION="namenode:9870 datanode:9864 hive-metastore-postgresql:5432"`: Specifies the dependent services as preconditions.
- Ports:
  - `9083:9083`: Exposes the Hive Metastore port.

### Hive Metastore PostgreSQL

The Hive Metastore PostgreSQL service is based on the `bde2020/hive-metastore-postgresql:2.3.0` image. It is configured with the following properties:

- Container name: `hive-metastore-postgresql`

## Presto

The Presto service is based on the `shawnzhu/prestodb:0.181` image. It is configured with the following properties:

- Container name: `presto-coordinator`
- Ports:
  - `8089:8089`: Exposes the Presto coordinator port.

## Database Engine

The Database Engine service is a custom-built service defined in the `db_engine/Dockerfile`. It is configured with the following properties:

- Image: `flask-ipfs-app`
- Container name: `flask_ipfs_interface`
- Restart policy: `unless-stopped`
- Ports:
  - `5000:5000`: Exposes the Flask application port.
- Environment variables:
  - `IPFS_API_URL=http://ipfs:5001`: Specifies the IPFS API URL.
  - `SPARK_MASTER=spark://spark-master:7077`: Specifies the Spark Master URL.
- Depends on:
  - `ipfs`
  - `spark-master`
  - `hive-server`
  - `hive-metastore`

<Admonition type="info">
  The `docker-compose.yml` file also defines several volumes used by the
  services to persist data. These volumes include `hadoop_namenode`,
  `hadoop_datanode`, `hadoop_historyserver`, `ipfs_data`, and
  `p2p_network_service`.
</Admonition>

This Docker Compose configuration sets up a comprehensive environment for the QueryAPIKernel solution, including IPFS, Hadoop, Spark, Hive, Presto, and a custom database engine. Each service is properly configured with the necessary images, volumes, ports, environment variables, and dependencies to ensure a functional and integrated system.
